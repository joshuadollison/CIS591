{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee0daedc-086f-4fe7-b251-c8cee7540e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 4. Logistic Regression\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     68\u001b[0m log_reg \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     69\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[1;32m     70\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m))\n\u001b[1;32m     71\u001b[0m ])\n\u001b[0;32m---> 73\u001b[0m \u001b[43mlog_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m y_pred_lr \u001b[38;5;241m=\u001b[39m log_reg\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/pipeline.py:660\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    655\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    656\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    657\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    658\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    659\u001b[0m         )\n\u001b[0;32m--> 660\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:1014\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[1;32m   1013\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m-> 1014\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:649\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    644\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt check \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_container\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sparse matrix for nan or inf.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    646\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    647\u001b[0m         )\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43msparse_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# TODO: Remove when the minimum version of SciPy supported is 1.12\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# With SciPy sparse arrays, conversion from DIA format to COO, CSR, or BSR\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# triggers the use of `np.int64` indices even if the data is such that it could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# algorithms support large indices, the following code downcasts to `np.int32`\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# indices when it's safe to do so.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m changed_format:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;66;03m# accept_sparse is specified to a specific format and a conversion occurred\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"claude.csv\")\n",
    "\n",
    "# Severity mapping\n",
    "severity_mapping = {\n",
    "    \"No Injury\": \"None\",\n",
    "    \"Possible Injury\": \"Minor\",\n",
    "    \"Non Incapacitating Injury\": \"Minor\",\n",
    "    \"Suspected Minor Injury\": \"Minor\",\n",
    "    \"Incapacitating Injury\": \"Severe\",\n",
    "    \"Suspected Serious Injury\": \"Severe\",\n",
    "    \"Fatal\": \"Severe\"\n",
    "}\n",
    "df[\"severity_class\"] = df[\"injury_severity\"].map(severity_mapping)\n",
    "\n",
    "# Binary target: Severe = 1, else 0\n",
    "df[\"target\"] = (df[\"severity_class\"] == \"Severe\").astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Feature selection\n",
    "# -----------------------------\n",
    "# Drop columns we shouldn't use (like IDs, raw severity, target, etc.)\n",
    "drop_cols = [\"injury_severity\", \"severity_class\", \"target\"]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Identify categorical vs numeric\n",
    "# -----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Preprocess: OneHot for categorical, passthrough numeric\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Extract odds ratios\n",
    "ohe = log_reg.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
    "cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([cat_features, num_cols])\n",
    "\n",
    "coefs = log_reg.named_steps[\"classifier\"].coef_[0]\n",
    "odds_ratios = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "}).sort_values(\"odds_ratio\", ascending=False)\n",
    "\n",
    "# Plot odds ratios (top 15)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=odds_ratios.head(15), x=\"odds_ratio\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Logistic Regression Odds Ratios (Severe Crash Likelihood)\")\n",
    "plt.xlabel(\"Odds Ratio (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Random Forest\n",
    "# -----------------------------\n",
    "rf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Feature importances\n",
    "importances = rf.named_steps[\"classifier\"].feature_importances_\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot top 15\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=feat_importances.head(15), x=\"importance\", y=\"feature\", palette=\"magma\")\n",
    "plt.title(\"Top 15 Random Forest Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7dc53-00fc-4a87-aa8f-41b33a95e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# 6. PCA Visualization\n",
    "# -----------------------------\n",
    "\n",
    "# Encode categorical features (same as before, but output array for PCA)\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# Scale features\n",
    "X_scaled = StandardScaler(with_mean=False).fit_transform(X_encoded)\n",
    "\n",
    "# PCA to 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Build plotting DataFrame\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"severity_ratio\"] = y  # 1 = severe, 0 = not severe\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=pca_df, \n",
    "    x=\"PC1\", y=\"PC2\",\n",
    "    hue=\"severity_ratio\", \n",
    "    palette={0:\"skyblue\", 1:\"darkred\"}, \n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(\"PCA of Intersections by Severity Class\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "plt.legend(title=\"Severity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fb480-b0d3-4a7f-b8b3-a6b516b6ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"claude.csv\")\n",
    "\n",
    "# Map severities\n",
    "severity_mapping = {\n",
    "    \"No Injury\": \"None\",\n",
    "    \"Possible Injury\": \"Minor\",\n",
    "    \"Non Incapacitating Injury\": \"Minor\",\n",
    "    \"Suspected Minor Injury\": \"Minor\",\n",
    "    \"Incapacitating Injury\": \"Severe\",\n",
    "    \"Suspected Serious Injury\": \"Severe\",\n",
    "    \"Fatal\": \"Severe\"\n",
    "}\n",
    "df[\"severity_class\"] = df[\"injury_severity\"].map(severity_mapping)\n",
    "\n",
    "# Binary target\n",
    "df[\"target\"] = (df[\"severity_class\"] == \"Severe\").astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Features & target\n",
    "# -----------------------------\n",
    "drop_cols = [\"injury_severity\", \"severity_class\", \"target\"]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Preprocessor with Imputation\n",
    "# -----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "        ]), num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Extract odds ratios\n",
    "ohe = log_reg.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([cat_features, num_cols])\n",
    "\n",
    "coefs = log_reg.named_steps[\"classifier\"].coef_[0]\n",
    "odds_ratios = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "}).sort_values(\"odds_ratio\", ascending=False)\n",
    "\n",
    "# Plot odds ratios\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=odds_ratios.head(15), x=\"odds_ratio\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Logistic Regression Odds Ratios\")\n",
    "plt.xlabel(\"Odds Ratio (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Random Forest\n",
    "# -----------------------------\n",
    "rf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Feature importances\n",
    "importances = rf.named_steps[\"classifier\"].feature_importances_\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=feat_importances.head(15), x=\"importance\", y=\"feature\", palette=\"magma\")\n",
    "plt.title(\"Top 15 Random Forest Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. PCA Visualization\n",
    "# -----------------------------\n",
    "# Full encoded + imputed data\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# Scale\n",
    "X_scaled = StandardScaler(with_mean=False).fit_transform(X_encoded)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"severity\"] = y\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=pca_df, x=\"PC1\", y=\"PC2\",\n",
    "    hue=\"severity\", palette={0:\"skyblue\", 1:\"darkred\"},\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(\"PCA Projection by Severity\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "plt.legend(title=\"Severe (1) vs Non-Severe (0)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8335cbf-9fea-4111-a0e0-d21a66f487fc",
   "metadata": {},
   "source": [
    "### Correction 3\n",
    "Tightly bound features <br/>\n",
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141c58f-4c16-466a-830f-fd597d29f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"claude.csv\")\n",
    "\n",
    "severity_mapping = {\n",
    "    \"No Injury\": \"None\",\n",
    "    \"Possible Injury\": \"Minor\",\n",
    "    \"Non Incapacitating Injury\": \"Minor\",\n",
    "    \"Suspected Minor Injury\": \"Minor\",\n",
    "    \"Incapacitating Injury\": \"Severe\",\n",
    "    \"Suspected Serious Injury\": \"Severe\",\n",
    "    \"Fatal\": \"Severe\"\n",
    "}\n",
    "df[\"severity_class\"] = df[\"injury_severity\"].map(severity_mapping)\n",
    "df[\"target\"] = (df[\"severity_class\"] == \"Severe\").astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Feature selection (drop outcome-like fields)\n",
    "# -----------------------------\n",
    "drop_cols = [\n",
    "    \"injury_severity\", \"severity_class\", \"target\",\n",
    "    \"total_injuries\", \"total_fatalities\", \"incident_id\", \"object_id\", \"Unnamed: 0\"\n",
    "]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Train/Test split + SMOTE\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "        ]), num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encode + SMOTE on training data only\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_enc, y_train)\n",
    "\n",
    "# Get feature names\n",
    "ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([cat_features, num_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Logistic Regression (Balanced)\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "log_reg.fit(X_train_bal, y_train_bal)\n",
    "y_pred_lr = log_reg.predict(X_test_enc)\n",
    "\n",
    "print(\"Logistic Regression Report (Balanced + SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "coefs = log_reg.coef_[0]\n",
    "odds_ratios = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "}).sort_values(\"odds_ratio\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=odds_ratios.head(15), x=\"odds_ratio\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Logistic Regression Odds Ratios (Balanced)\")\n",
    "plt.xlabel(\"Odds Ratio (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Random Forest (Balanced)\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, random_state=42, class_weight=\"balanced\"\n",
    ")\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "y_pred_rf = rf.predict(X_test_enc)\n",
    "\n",
    "print(\"Random Forest Report (Balanced + SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=feat_importances.head(15), x=\"importance\", y=\"feature\", palette=\"magma\")\n",
    "plt.title(\"Top 15 Random Forest Feature Importances (Balanced)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. PCA Visualization (Balanced)\n",
    "# -----------------------------\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_scaled = scaler.fit_transform(X_train_bal)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"severity\"] = y_train_bal\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=pca_df, x=\"PC1\", y=\"PC2\",\n",
    "    hue=\"severity\", palette={0:\"skyblue\", 1:\"darkred\"}, alpha=0.5\n",
    ")\n",
    "plt.title(\"PCA Projection After Balancing\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "plt.legend(title=\"Severity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53986999-7d28-474c-aa9a-4551e4e20b8d",
   "metadata": {},
   "source": [
    "### Better focus on intersection characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362c52d-41b7-4230-a2b4-1da35a364227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map severity\n",
    "severity_mapping = {\n",
    "    \"No Injury\": \"None\",\n",
    "    \"Possible Injury\": \"Minor\",\n",
    "    \"Non Incapacitating Injury\": \"Minor\",\n",
    "    \"Suspected Minor Injury\": \"Minor\",\n",
    "    \"Incapacitating Injury\": \"Severe\",\n",
    "    \"Suspected Serious Injury\": \"Severe\",\n",
    "    \"Fatal\": \"Severe\"\n",
    "}\n",
    "df[\"severity_class\"] = df[\"injury_severity\"].map(severity_mapping)\n",
    "df[\"is_severe\"] = (df[\"severity_class\"] == \"Severe\").astype(int)\n",
    "\n",
    "# Helper flags using both unit_1 and unit_2\n",
    "def flag_any(row, cols, keyword):\n",
    "    return int(any(str(row[c]).lower().find(keyword.lower()) >= 0 for c in cols if pd.notna(row[c])))\n",
    "\n",
    "df[\"alcohol_flag\"] = df.apply(lambda r: flag_any(r, [\"alcohol_use_1\", \"alcohol_use_2\"], \"yes\"), axis=1)\n",
    "df[\"drug_flag\"]    = df.apply(lambda r: flag_any(r, [\"drug_use_1\", \"drug_use_2\"], \"yes\"), axis=1)\n",
    "df[\"ped_flag\"]     = df.apply(lambda r: flag_any(r, [\"unit_type_1\", \"unit_type_2\"], \"pedestrian\"), axis=1)\n",
    "df[\"bike_flag\"]    = df.apply(lambda r: flag_any(r, [\"unit_type_1\", \"unit_type_2\"], \"pedalcyclist\"), axis=1)\n",
    "df[\"speed_flag\"]   = df.apply(lambda r: flag_any(r, [\"violation_1\", \"violation_2\"], \"speed\"), axis=1)\n",
    "\n",
    "# Night flag\n",
    "df[\"night_flag\"] = df[\"light_condition\"].astype(str).str.contains(\"Dark\", case=False, na=False).astype(int)\n",
    "\n",
    "# Average driver age (take mean of available ages for unit 1 & 2)\n",
    "df[\"avg_age_event\"] = df[[\"age_1\", \"age_2\"]].mean(axis=1)\n",
    "\n",
    "# Aggregate to intersections\n",
    "intersection_stats = df.groupby([\"street_name\", \"cross_street\"]).agg(\n",
    "    accident_count = (\"injury_severity\", \"count\"),\n",
    "    severe_count   = (\"is_severe\", \"sum\"),\n",
    "    alcohol_pct    = (\"alcohol_flag\", \"mean\"),\n",
    "    drug_pct       = (\"drug_flag\", \"mean\"),\n",
    "    ped_pct        = (\"ped_flag\", \"mean\"),\n",
    "    bike_pct       = (\"bike_flag\", \"mean\"),\n",
    "    night_pct      = (\"night_flag\", \"mean\"),\n",
    "    speed_pct      = (\"speed_flag\", \"mean\"),\n",
    "    avg_age        = (\"avg_age_event\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Severity ratio\n",
    "intersection_stats[\"severity_ratio\"] = (\n",
    "    intersection_stats[\"severe_count\"] / intersection_stats[\"accident_count\"]\n",
    ")\n",
    "\n",
    "# Preview top intersections\n",
    "print(intersection_stats.sort_values(\"accident_count\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5678f2e-86fc-4c2c-a0fb-fda9fd1da8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"claude.csv\")\n",
    "\n",
    "# Severity mapping\n",
    "severity_mapping = {\n",
    "    \"No Injury\": \"None\",\n",
    "    \"Possible Injury\": \"Minor\",\n",
    "    \"Non Incapacitating Injury\": \"Minor\",\n",
    "    \"Suspected Minor Injury\": \"Minor\",\n",
    "    \"Incapacitating Injury\": \"Severe\",\n",
    "    \"Suspected Serious Injury\": \"Severe\",\n",
    "    \"Fatal\": \"Severe\"\n",
    "}\n",
    "df[\"severity_class\"] = df[\"injury_severity\"].map(severity_mapping)\n",
    "df[\"target\"] = (df[\"severity_class\"] == \"Severe\").astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Features / target\n",
    "# -----------------------------\n",
    "drop_cols = [\"injury_severity\", \"severity_class\", \"target\", \n",
    "             \"total_injuries\", \"total_fatalities\", \"incident_id\", \n",
    "             \"object_id\", \"Unnamed: 0\"]\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "X = df[feature_cols]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Preprocessing + SMOTE\n",
    "# -----------------------------\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "        ]), num_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encode train/test\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_enc, y_train)\n",
    "\n",
    "# Feature names\n",
    "ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([cat_features, num_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "log_reg.fit(X_train_bal, y_train_bal)\n",
    "y_pred_lr = log_reg.predict(X_test_enc)\n",
    "\n",
    "print(\"Logistic Regression Report (Crash-Level + SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "coefs = log_reg.coef_[0]\n",
    "odds_ratios = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"odds_ratio\": np.exp(coefs)\n",
    "}).sort_values(\"odds_ratio\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=odds_ratios.head(15), x=\"odds_ratio\", y=\"feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Logistic Regression Odds Ratios (Severe Crash Likelihood)\")\n",
    "plt.xlabel(\"Odds Ratio (log scale)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Random Forest\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight=\"balanced\")\n",
    "rf.fit(X_train_bal, y_train_bal)\n",
    "y_pred_rf = rf.predict(X_test_enc)\n",
    "\n",
    "print(\"Random Forest Report (Crash-Level + SMOTE):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feat_importances = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=feat_importances.head(15), x=\"importance\", y=\"feature\", palette=\"magma\")\n",
    "plt.title(\"Top 15 Random Forest Feature Importances (Severe Crash Likelihood)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-py312]",
   "language": "python",
   "name": "conda-env-anaconda3-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
